# Default model to use with Ollama
model_name: "tinyllama"
temperature: 0.0
base_url: "http://localhost:11434"

# Optional advanced parameters
num_ctx: 8192
num_predict: 2048
stop:
  - "Observation:"
  - "Final Answer:"
